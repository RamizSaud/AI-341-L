{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "9d7pXwFTqU2i"
   },
   "outputs": [],
   "source": [
    "new_sentences = [\n",
    "    \"This movie was amazing!\",\n",
    "    \"I didn't like the product I received.\",\n",
    "    \"The weather is nice today.\",\n",
    "    \"The food at that restaurant is terrible.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "uxcPS5FVqopV"
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "flzUlgm3qqzU",
    "outputId": "0496ca94-2e31-4475-aa3c-865d91f8efae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0RsjtpfgqtyM",
    "outputId": "c699850e-560f-4e84-b60a-97d7608dcbe9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt', download_dir='/usr/share/nltk_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KbpAQjR9qaJE",
    "outputId": "3dbebf8a-1dcb-46e0-be2d-cb979ad3b38a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Tokenization\n",
    "    words = word_tokenize(text)#to split text\n",
    "\n",
    "    # Remove punctuation and convert to lowercase\n",
    "    words = [word.lower() for word in words if word.isalpha()]\n",
    "\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    return \" \".join(words)\n",
    "\n",
    "preprocessed_sentences = [preprocess_text(sentence) for sentence in new_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "26-6t-zAqc0k",
    "outputId": "8a244065-3664-4648-bb61-9730f1d0b585"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1/1 [==============================] - 6s 6s/step - loss: 0.6926 - accuracy: 0.5000 - val_loss: 0.6868 - val_accuracy: 0.7500\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 0.6838 - accuracy: 1.0000 - val_loss: 0.6822 - val_accuracy: 0.5000\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 0.6747 - accuracy: 1.0000 - val_loss: 0.6774 - val_accuracy: 0.5000\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 0.6648 - accuracy: 1.0000 - val_loss: 0.6722 - val_accuracy: 0.5000\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 0.6535 - accuracy: 1.0000 - val_loss: 0.6666 - val_accuracy: 0.5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7be7ecd90640>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Training data\n",
    "training_texts = [\n",
    "    \"This is a great product. I love it!\",\n",
    "    \"The service was terrible. I'm very disappointed.\",\n",
    "    \"The weather is fantastic today.\",\n",
    "    \"The movie was boring and a waste of time\",\n",
    "]\n",
    "\n",
    "# Testing data\n",
    "testing_texts = [\n",
    "    \"I absolutely loved it!\",\n",
    "    \"This was a waste of money\",\n",
    "    \"The weather is amazing today.\",\n",
    "    \"The book was so boring\",\n",
    "]\n",
    "\n",
    "# Sentiment labels\n",
    "y_train = [\n",
    "    \"positive\",\n",
    "    \"negative\",\n",
    "    \"positive\",\n",
    "    \"negative\",\n",
    "]\n",
    "\n",
    "y_test = [\n",
    "    \"positive\",\n",
    "    \"negative\",\n",
    "    \"positive\",\n",
    "    \"negative\",\n",
    "]\n",
    "\n",
    "# Encode sentiment labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "# Tokenize the text data\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(training_texts)\n",
    "X_train = tokenizer.texts_to_sequences(training_texts)\n",
    "X_test = tokenizer.texts_to_sequences(testing_texts)\n",
    "\n",
    "# Pad sequences to have the same length\n",
    "X_train = pad_sequences(X_train, maxlen=100)\n",
    "X_test = pad_sequences(X_test, maxlen=100)\n",
    "\n",
    "# Build and compile the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(5000, 128, input_length=100))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train_encoded, validation_data=(X_test, y_test_encoded), epochs=5, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JLPQMQwnq1ou",
    "outputId": "82917b30-a40c-4984-a010-f1f1c7f5b13c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 76ms/step - loss: 0.6666 - accuracy: 0.5000\n",
      "Accuracy: 50.00%\n"
     ]
    }
   ],
   "source": [
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test_encoded)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W0AJ85pdtFLD"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
